# EMO-Reasoning: Benchmarking Emotional Reasoning Capabilities in Spoken Dialogue Systems

This project is divided into two main parts:

1.  **CSER Model**: A BiLSTM-based model that predicts continuous emotional dimensions (Valence, Arousal, Dominance) from audio.
2.  **Emotional Reasoning Evaluation**: Scripts to assess a conversational agent's emotional appropriateness using both categorical and continuous metrics (ECS, EBS, ESS).

## Quickstart

### 1. Setup

Clone the repository and install the required packages.

```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
pip install -r requirements.txt
```

Next, download the **WavLM-Large** model checkpoint from [Microsoft's official repository](https://github.com/microsoft/unilm/tree/master/wavlm) and place it in a known directory (e.g., `./pretrained_models/`).

Then, download the **MSP-Conversation** dataset from [this link](https://www.isca-archive.org/interspeech_2020/martinezlucas20_interspeech.pdf).

### 2. Data Format

The model was trained on the MSP-Conversation dataset. If you are using your own data, please format it as follows:

-   **Audio**: Single-channel (mono) `.wav` files.
-   **Labels (for training CSER)**: `.csv` files with the same name as their audio file. Must contain `arousal`, `valence`, and `dominance` columns, with one row per second of audio.
-   **Labels (for evaluation)**:
    -   **Categorical**: `.csv` files with `Filename` and `Emotion` columns.
    -   **Continuous**: `.csv` files with `arousal`, `valence`, and `dominance` columns (typically generated by the CSER model).

## Data Preprocessing (MSP-Conversation)

The CSER model was trained on the **MSP-Conversation** dataset. If you are using this dataset, you must first process the raw audio and annotations into a usable format.

The following scripts assume the raw MSP-Conversation data is structured like this:

```
/path/to/MSP-Conversation_raw/
├── Audio/Audio/
├── Annotations/Annotations/
└── Time_Labels/Time_Labels/
```

### Step 1: Segment Audio into Turns

Use `process_audio.py` to segment the full conversation recordings into smaller, turn-level `.wav` files.

```bash
python process_audio.py \
  --base_audio_dir /path/to/MSP-Conversation_raw/Audio/Audio \
  --time_labels_dir /path/to/MSP-Conversation_raw/Time_Labels/Time_Labels \
  --output_dir /path/to/segmented_audio
```

### Step 2: Process Annotations

Use `process_annotations.py` to convert the raw, multi-annotator text files into clean, second-by-second `.csv` label files that correspond to the segmented audio.

```bash
python process_annotations.py \
  --segmented_audio_dir /path/to/segmented_audio \
  --annotations_dir /path/to/MSP-Conversation_raw/Annotations/Annotations \
  --output_dir /path/to/processed_labels
```

The directories created by these two steps will serve as the input for training the CSER model.

## Part 1: Continuous Speech Emotion Recognition (CSER)

This workflow trains a model to predict VAD values from audio.

### Step 1: Extract Features

First, convert your `.wav` files into feature vectors using `extract_feature.py`. This script uses WavLM to generate features and averages them into 1-second chunks.

```bash
python extract_feature.py \
  --audio_dir /path/to/wav/train \
  --output_dir /path/to/features/train \
  --wavlm_path ./pretrained_models/WavLM-Large.pt
```

### Step 2: Train the CSER Model

Train the BiLSTM model on the pre-extracted features.

```bash
python train.py \
  --train_feature_dir /path/to/features/train \
  --train_label_dir /path/to/labels/train \
  --val_feature_dir /path/to/features/dev \
  --val_label_dir /path/to/labels/dev \
  --ckpt_dir ./checkpoints/cser_model
```

### Step 3: Inference

Use a trained model to predict VAD values for new audio.

-   **Single file with visualization**:
    ```bash
    python inference.py \
      --feature_path /path/to/features/dev/sample_features.pt \
      --label_path /path/to/labels/dev/sample.csv \
      --model_path ./checkpoints/cser_model/epoch_200.pt \
      --output_path ./prediction.pdf
    ```

-   **Batch inference for a directory**:
    ```bash
    python evaluate.py \
      --input_dir /path/to/new_audio_files \
      --output_dir /path/to/save/predictions \
      --wavlm_path ./pretrained_models/WavLM-Large.pt \
      --cser_model_path ./checkpoints/cser_model/epoch_200.pt
    ```

## Part 2: Emotional Reasoning Evaluation

Use these scripts to score the emotional quality of an agent's responses.

### 1. Categorical Evaluation

Scores responses based on a predefined appropriateness table (e.g., 'happy' response to 'happy' user).

```bash
python categorical_evaluate.py \
  --user_emotion_dir /path/to/user_emotions \
  --agent_response_dir /path/to/agent_responses_parent_folder \
  --output_dir /path/to/eval_results \
  --models LLaMA-Omni Freeze-omni
```

The `--agent_response_dir` should contain subdirectories for each model being evaluated.

### 2. Continuous Evaluation (ERS, ECS, EBS, ESS)

Calculates advanced interaction metrics from continuous VAD predictions.

-   **Single-Turn Evaluation**:

    ```bash
    python continuous_evaluate.py single-turn \
      --user_dir /path/to/user_vad_csvs \
      --agent_dir /path/to/agent_vad_csvs \
      --output_csv /path/to/single_turn_scores.csv
    ```

-   **Multi-Turn Evaluation**:
    ```bash
    python continuous_evaluate.py multi-turn \
      --dialogues_dir /path/to/multi_turn_dialogue_folders \
      --output_csv /path/to/multi_turn_scores.csv
    ```
    This mode requires a specific directory structure where each dialogue is in its own subfolder.
